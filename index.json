[{"authors":["admin"],"categories":null,"content":"I was born on August 10, 1991 in Vibo Valentia, Italy. I received the Ph.D. in Information Engineering and Science at the Department of Information Engineering and Mathematics, University of Siena, under the supervision of Prof. Andrea Garulli and Prof. Antonio Giannitrapani with a thesis entitled Distributed Algorithms for Set Membership Estimation and Constrained Nonconvex Optimization.\nSince December 2018, I am a Postdoctoral Researcher at the Department of Electrical, Electronic, and Information Engineering “Guglielmo Marconi”, University of Bologna, Italy.\nCurrently, my main research activities include:\n design and analysis of distributed algorithms for  convex/nonconvex optimization stochastic and online optimization big-data applications supervised/semi-supervised machine learning, learning from constraints reinforcement learning and optimal control personalized optimization and problems with unknown objective set-membership estimation   collective learning local-propagation-based neural networks for architecture learning  I serve as reviewer for various journal and conferences, including IEEE TNNLS, Automatica, IEEE TAC, IEEE LCSS, IEEE TCNS, IEEE CDC, IEEE ACC\n","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"598b63dd58b43bce02403646f240cd3c","permalink":"https://francescofarina.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"author","summary":"I was born on August 10, 1991 in Vibo Valentia, Italy. I received the Ph.D. in Information Engineering and Science at the Department of Information Engineering and Mathematics, University of Siena, under the supervision of Prof. Andrea Garulli and Prof. Antonio Giannitrapani with a thesis entitled Distributed Algorithms for Set Membership Estimation and Constrained Nonconvex Optimization.\nSince December 2018, I am a Postdoctoral Researcher at the Department of Electrical, Electronic, and Information Engineering “Guglielmo Marconi”, University of Bologna, Italy.","tags":null,"title":"Francesco Farina","type":"author"},{"authors":["Francesco Farina","Giuseppe Notarstefano"],"categories":null,"content":"","date":1583017200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583017200,"objectID":"3bce95a18b40ea4fff43c966b56d458c","permalink":"https://francescofarina.github.io/publication/farina-2020-lcss/","publishdate":"2020-03-01T00:00:00+01:00","relpermalink":"/publication/farina-2020-lcss/","section":"publication","summary":"The recently developed Distributed Block Proximal Method, for solving stochastic big-data convex optimization problems, is studied in this paper under the assumption of constant stepsizes and strongly convex (possibly non-smooth) local objective functions. This class of problems arises in many learning and classification problems in which, for example, strongly-convex regularizing functions are included in the objective function, the decision variable is extremely high dimensional, and large datasets are employed. The algorithm produces local estimates by means of block-wise updates and communication among the agents. The expected distance from the (global) optimum, in terms of cost value, is shown to decay linearly to a constant value which is proportional to the selected local stepsizes. A numerical example involving a classification problem corroborates the theoretical results.","tags":["journal"],"title":"On the Linear Convergence Rate of the Distributed Block Proximal Method","type":"publication"},{"authors":["Francesco Farina","Giuseppe Notarstefano"],"categories":null,"content":"","date":1575154800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1575154800,"objectID":"b2cff0dc9744c37ea33a4df3dbf81573","permalink":"https://francescofarina.github.io/publication/farina-2019-block_cdc/","publishdate":"2019-12-01T00:00:00+01:00","relpermalink":"/publication/farina-2019-block_cdc/","section":"publication","summary":"This paper introduces a novel distributed algorithm over static directed graphs for solving big data convex optimization problems in which the dimension of the decision variable can be extremely high and the objective function can be nonsmooth. In the proposed algorithm nodes in the network update and communicate only blocks of their current solution estimate rather than the entire vector. The algorithm consists of two main steps: a consensus step and a subgradient update on a single block of the optimization variable (which is then broadcast to neighbors). Agents are shown to asymptotically achieve consensus by studying a block-wise consensus protocol over random graphs. Then convergence to the optimal cost is proven in expected value by exploiting the consensus of agents estimates and randomness of the algorithm. Finally, as a numerical example, a distributed linear classification problem is solved by means of the proposed algorithm.","tags":["conference"],"title":"A Randomized Block Subgradient Approach to Distributed Big Data Optimization","type":"publication"},{"authors":["Francesco Farina"],"categories":null,"content":"","date":1575154800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1575154800,"objectID":"1523e85dba8573a893faddf54befe5ba","permalink":"https://francescofarina.github.io/publication/farina-2019-collective/","publishdate":"2019-12-01T00:00:00+01:00","relpermalink":"/publication/farina-2019-collective/","section":"publication","summary":"In this paper, we introduce the concept of collective learning (CL) which exploits the notion of collective intelligence in the field of distributed semi-supervised learning. The proposed framework draws inspiration from the learning behavior of human beings, who alternate phases involving collaboration, confrontation and exchange of views with other consisting of studying and learning on their own. On this regard, CL comprises two main phases: a self-training phase in which learning is performed on local private (labeled) data only and a collective training phase in which proxy-labels are assigned to shared (unlabeled) data by means of a consensus-based algorithm. In the considered framework, heterogeneous systems can be connected over the same network, each with different computational capabilities and resources and everyone in the network may take advantage of the cooperation and will eventually reach higher performance with respect to those it can reach on its own. An extensive experimental campaign on an image classification problem emphasizes the properties of CL by analyzing the performance achieved by the cooperating agents.","tags":["preprint"],"title":"Collective Learning","type":"publication"},{"authors":["Andrea Camisa","Francesco Farina","Ivano Notarnicola","Giuseppe Notarstefano"],"categories":null,"content":"","date":1575154800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1575154800,"objectID":"57d056fd3c16455e47cdf63405d74378","permalink":"https://francescofarina.github.io/publication/farina-2019-primal-cdc/","publishdate":"2019-12-01T00:00:00+01:00","relpermalink":"/publication/farina-2019-primal-cdc/","section":"publication","summary":"In this paper, we consider a network of processors that want to cooperatively solve a large-scale, convex optimization problem. Each processor has knowledge of a local cost function that depends only on a local variable. The goal is to minimize the sum of the local costs, while making the variables satisfy both local constraints and a global coupling constraint. We propose a simple, fully distributed algorithm, that works in a random, time-varying communication model, where at each iteration multiple edges are randomly drawn from an underlying graph. The algorithm is interpreted as a primal decomposition scheme applied to an equivalent problem reformulation. Almost sure convergence to the optimal cost of the original problem is proven by resorting to approaches from block subgradient methods. Specifically, the communication structure is mapped to a block structure, where the blocks correspond to the graph edges and are randomly selected at each iteration. Moreover, an almost sure asymptotic primal recovery property, with no averaging mechanisms, is shown. A numerical example corroborates the theoretical analysis.","tags":["conference"],"title":"Distributed Constraint-Coupled Optimization over Random Time-Varying Graphs via Primal Decomposition and Block Subgradient Approaches","type":"publication"},{"authors":["Francesco Farina","Antonio Giannitrapani","Andrea Garulli"],"categories":null,"content":"","date":1575154800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1575154800,"objectID":"a08c21f1e46bca4f01b83b47c539bd37","permalink":"https://francescofarina.github.io/publication/farina-2019-set-cdc/","publishdate":"2019-12-01T00:00:00+01:00","relpermalink":"/publication/farina-2019-set-cdc/","section":"publication","summary":"Distributed estimation schemes are increasingly popular these days. A distributed algorithm, specifically tailored to recursive set membership estimation problems, was recently proposed and analyzed for networks featuring a static topology. It was shown that the agents’ estimates asymptotically converge to a common point lying in the intersection of all the agents’ feasible sets. In this paper, by building on recent results on constrained consensus, we prove convergence in the more challenging scenario of networks with time-varying topology. It is shown that convergence is guaranteed if the sequence of graphs is jointly strongly connected over finite-length time intervals. Moreover, an asynchronous version of the proposed algorithm is presented, whose convergence can be deduced from the previously obtained results.","tags":["conference"],"title":"Distributed set membership estimation with time-varying graph topology","type":"publication"},{"authors":null,"categories":null,"content":"","date":1573205894,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1573205894,"objectID":"a212de44449a894fb7f6b3c5ba8ca90f","permalink":"https://francescofarina.github.io/project/disropt/","publishdate":"2019-11-08T10:38:14+01:00","relpermalink":"/project/disropt/","section":"project","summary":"","tags":[],"title":"Disropt","type":"project"},{"authors":["Francesco Farina","Stefano Melacci","Andrea Garulli","Antonio Giannitrapani"],"categories":null,"content":"","date":1573167600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1573167600,"objectID":"cbc5ec65e627cdd2ee20a53380dab746","permalink":"https://francescofarina.github.io/publication/farina-2019-lfc/","publishdate":"2019-11-08T00:00:00+01:00","relpermalink":"/publication/farina-2019-lfc/","section":"publication","summary":"In this brief, the extension of the framework of Learning from Constraints (LfC) to a distributed setting where multiple parties, connected over the network, contribute to the learning process is studied. LfC relies on the generic notion of ``constraint'' to inject knowledge into the learning problem, and, due to its generality, it deals with possibly nonconvex constraints, enforced either in a hard or soft way. Motivated by recent progresses in the field of distributed and constrained nonconvex optimization, we apply the (distributed) asynchronous method of multipliers (ASYMM) to LfC. The study shows that such a method allows us to support scenarios where selected constraints (i.e., knowledge), data, and outcomes of the learning process can be locally stored in each computational node without being shared with the rest of the network, opening the road to further investigations into privacy-preserving LfC. Constraints act as a bridge between what is shared over the net and what is private to each node, and no central authority is required. We demonstrate the applicability of these ideas in two distributed real-world settings in the context of digit recognition and document classification.","tags":["journal"],"title":"Asynchronous Distributed Learning From Constraints","type":"publication"},{"authors":["Francesco Farina","Andrea Camisa","Andrea Testa","Ivano Notarnicola","Giuseppe Notarstefano"],"categories":null,"content":"","date":1572562800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572562800,"objectID":"a67507f0a42ab3735b96afc919fd931b","permalink":"https://francescofarina.github.io/publication/farina-2019-disropt/","publishdate":"2019-11-01T00:00:00+01:00","relpermalink":"/publication/farina-2019-disropt/","section":"publication","summary":"In this paper we introduce disropt, a Python package for distributed optimization over networks. We focus on cooperative set-ups in which an optimization problem must be solved by peer-to-peer processors (without central coordinators) that have access only to partial knowledge of the entire problem. To reflect this, agents in disropt are modeled as entities that are initialized with their local knowledge of the problem. Agents then run local routines and communicate with each other to solve the global optimization problem. A simple syntax has been designed to allow for an easy modeling of the problems. The package comes with many distributed optimization algorithms that are already embedded. Moreover, the package provides full-fledged functionalities for communication and local computation, which can be used to design and implement new algorithms. disropt is available at github.com/disropt/disropt under the GPL license, with a complete documentation and many examples.","tags":["preprint"],"title":"DISROPT: a Python Framework for Distributed Optimization","type":"publication"},{"authors":["T. L. Baldi","F. Farina","A. Garulli","A. Giannitrapani","D. Prattichizzo"],"categories":null,"content":"","date":1569880800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569880800,"objectID":"d8b9dd49313fa124d999ebe94f9c574e","permalink":"https://francescofarina.github.io/publication/baldi-2019-upper/","publishdate":"2019-10-01T00:00:00+02:00","relpermalink":"/publication/baldi-2019-upper/","section":"publication","summary":"","tags":["journal"],"title":"Upper Body Pose Estimation Using Wearable Inertial Sensors and Multiplicative Kalman Filter","type":"publication"},{"authors":["Francesco Farina","Giuseppe Notarstefano"],"categories":null,"content":"","date":1556661600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1556661600,"objectID":"8f1349dd5597ada6227fe96558804134","permalink":"https://francescofarina.github.io/publication/farina-2019-proximal/","publishdate":"2019-05-01T00:00:00+02:00","relpermalink":"/publication/farina-2019-proximal/","section":"publication","summary":"In this paper we introduce a class of novel distributed algorithms for solving stochastic big-data convex optimization problems over directed graphs. In the addressed set-up, the dimension of the decision variable can be extremely high and the objective function can be nonsmooth. The general algorithm consists of two main steps: a consensus step and an update on a single block of the optimization variable, which is then broadcast to neighbors. Three special instances of the proposed method, involving particular problem structures, are then presented. In the general case, the convergence of a dynamic consensus algorithm over random row stochastic matrices is shown. Then, the convergence of the proposed algorithm to the optimal cost is proven in expected value. Exact convergence is achieved when using diminishing (local) stepsizes, while approximate convergence is attained when constant stepsizes are employed. In both cases, the convergence rate is shown to be sublinear. Finally, a numerical example involving a distributed classification problem is provided to corroborate the theoretical results.","tags":["preprint"],"title":"Randomized Block Proximal Methods for Distributed Stochastic Big-Data Optimization","type":"publication"},{"authors":["Francesco Farina","Andrea Garulli","Antonio Giannitrapani","Giuseppe Notarstefano"],"categories":null,"content":"","date":1554069600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554069600,"objectID":"1f59115207f305b3e8dfee71a2013eb0","permalink":"https://francescofarina.github.io/publication/farina-2019-distributed/","publishdate":"2019-04-01T00:00:00+02:00","relpermalink":"/publication/farina-2019-distributed/","section":"publication","summary":"This paper presents a fully asynchronous and distributed approach for tackling optimization problems in which both the objective function and the constraints may be nonconvex. In the considered network setting each node is active upon triggering of a local timer and has access only to a portion of the objective function and to a subset of the constraints. In the proposed technique, based on the method of multipliers, each node performs, when it wakes up, either a descent step on a local augmented Lagrangian or an ascent step on the local multiplier vector. Nodes realize when to switch from the descent step to the ascent one through an asynchronous distributed logic-AND, which detects when all the nodes have reached a predefined tolerance in the minimization of the augmented Lagrangian. It is shown that the resulting distributed algorithm is equivalent to a block coordinate descent for the minimization of the global augmented Lagrangian. This allows one to extend the properties of the centralized method of multipliers to the considered distributed framework. Two application examples are presented to validate the proposed approach: a distributed source localization problem and the parameter estimation of a neural network.","tags":["journal"],"title":"A distributed asynchronous method of multipliers for constrained nonconvex optimization","type":"publication"},{"authors":["Francesco Farina","Andrea Garulli","Antonio Giannitrapani"],"categories":null,"content":"","date":1546297200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546297200,"objectID":"8a29232d252f02331eb2003e3a19df37","permalink":"https://francescofarina.github.io/publication/farina-2018-distributed/","publishdate":"2019-01-01T00:00:00+01:00","relpermalink":"/publication/farina-2018-distributed/","section":"publication","summary":"This work addresses the distributed estimation problem in a set membership framework. The agents of a network collect measurements which are affected by bounded errors, thus implying that the unknown parameters to be estimated belong to a suitable feasible set. Two distributed algorithms are considered, based on projections of the estimate of each agent onto its local feasible set. The main contribution of the paper is to show that such algorithms are asymptotic interpolatory estimators, i.e. they converge to an element of the global feasible set, under the assumption that the feasible set associated to each measurement is convex. The proposed techniques are demonstrated on a distributed linear regression estimation problem.","tags":["journal"],"title":"Distributed interpolatory algorithms for set membership estimation","type":"publication"},{"authors":["Francesco Farina","Andrea Testa","Giuseppe Notarstefano"],"categories":null,"content":"","date":1546297200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546297200,"objectID":"546b7370e8026cdd53a6f04be21d523c","permalink":"https://francescofarina.github.io/publication/farina-2019-submodular/","publishdate":"2019-01-01T00:00:00+01:00","relpermalink":"/publication/farina-2019-submodular/","section":"publication","summary":"In this paper we deal with a network of computing agents with local processing and neighboring communication capabilities that aim at solving (without any central unit) a submodular optimization problem. The cost function is the sum of many local submodular functions and each agent in the network has access to one function in the sum only. In this emphdistributed set-up, in order to preserve their own privacy, agents communicate with neighbors but do not share their local cost functions. We propose a distributed algorithm in which agents resort to the Lovàsz extension of their local submodular functions and perform local updates and communications in terms of single blocks of the entire optimization variable. Updates are performed by means of a greedy algorithm which is run only until the selected block is computed, thus resulting in a reduced computational burden. The proposed algorithm is shown to converge in expected value to the optimal cost of the problem, and an approximate solution to the submodular problem is retrieved by a thresholding operation. As an application, we consider a distributed image segmentation problem in which each agent has access only to a portion of the entire image. While agent cannot segment the entire image on their own, they correctly complete the task by cooperating through the proposed distributed algorithm.","tags":["preprint"],"title":"Distributed Submodular Minimization via Block-Wise Updates and Communications","type":"publication"},{"authors":["Francesco Farina","Andrea Garulli","Antonio Giannitrapani","Giuseppe Notarstefano"],"categories":null,"content":"","date":1527804000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1527804000,"objectID":"5c65348108d35d60f5a7285335bb54a3","permalink":"https://francescofarina.github.io/publication/farina-2018-asynchronous/","publishdate":"2018-06-01T00:00:00+02:00","relpermalink":"/publication/farina-2018-asynchronous/","section":"publication","summary":"This paper addresses a class of constrained optimization problems over networks in which local cost functions and constraints can be nonconvex. We propose an asynchronous distributed optimization algorithm, relying on the centralized Method of Multipliers, in which each node wakes up in an uncoordinated fashion and performs either a descent step on a local Augmented Lagrangian or an ascent step on the local multiplier vector. These two phases are regulated by a distributed logic-AND, which allows nodes to understand when the descent on the (whole) Augmented Lagrangian is sufficiently small. We show that this distributed algorithm is equivalent to a block coordinate descent algorithm for the minimization of the Augmented Lagrangian followed by an update of the whole multiplier vector. Thus, the proposed algorithm inherits the convergence properties of the Method of Multipliers.","tags":["conference"],"title":"Asynchronous Distributed Method of Multipliers for Constrained Nonconvex optimization","type":"publication"},{"authors":["Mirko Leomanni","Andrea Garulli","Antonio Giannitrapani","Francesco Farina","Fabrizio Scortecci"],"categories":null,"content":"","date":1490997600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1490997600,"objectID":"6075ec25474ff8b9bcd1692e0351005f","permalink":"https://francescofarina.github.io/publication/leomanni-2017-minimum/","publishdate":"2017-04-01T00:00:00+02:00","relpermalink":"/publication/leomanni-2017-minimum/","section":"publication","summary":"Maintaining the attitude of a spacecraft precisely aligned to a given orientation is crucial for commercial and scientific space missions. The problem becomes challenging when on/off thrusters are employed instead of momentum exchange devices due to, e.g., wheel failures or power limitations. In this case, the attitude control system must enforce an oscillating motion about the setpoint, so as to minimize the switching frequency of the actuators, while guaranteeing a predefined pointing accuracy and rejecting the external disturbances. This paper develops a three-axis attitude control scheme for this problem, accounting for the limitations imposed by the thruster technology. The proposed technique is able to track both the period and the phase of periodic oscillations along the rotational axes, which is instrumental to minimize the switching frequency in the presence of input coupling. Two simulation case studies of a geostationary mission and a low Earth orbit mission are reported, showing that the proposed controller can effectively deal with both constant and time-varying disturbance torques.","tags":["journal"],"title":"Minimum Switching Thruster Control for Spacecraft Precision Pointing","type":"publication"},{"authors":["Francesco AND Fontanelli, Daniele AND Garulli, Andrea AND Giannitrapani, Antonio AND Prattichizzo, Domenico Farina"],"categories":null,"content":"","date":1483225200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483225200,"objectID":"714b9fa4ced7ad9cc31a2205d1dfd527","permalink":"https://francescofarina.github.io/publication/farina-2017-walking/","publishdate":"2017-01-01T00:00:00+01:00","relpermalink":"/publication/farina-2017-walking/","section":"publication","summary":"Human motion models are finding an increasing number of novel applications in many different fields, such as building design, computer graphics and robot motion planning. The Social Force Model is one of the most popular alternatives to describe the motion of pedestrians. By resorting to a physical analogy, individuals are assimilated to point-wise particles subject to social forces which drive their dynamics. Such a model implicitly assumes that humans move isotropically. On the contrary, empirical evidence shows that people do have a preferred direction of motion, walking forward most of the time. Lateral motions are observed only in specific circumstances, such as when navigating in overcrowded environments or avoiding unexpected obstacles. In this paper, the Headed Social Force Model is introduced in order to improve the realism of the trajectories generated by the classical Social Force Model. The key feature of the proposed approach is the inclusion of the pedestrians’ heading into the dynamic model used to describe the motion of each individual. The force and torque representing the model inputs are computed as suitable functions of the force terms resulting from the traditional Social Force Model. Moreover, a new force contribution is introduced in order to model the behavior of people walking together as a single group. The proposed model features high versatility, being able to reproduce both the unicycle-like trajectories typical of people moving in open spaces and the point-wise motion patterns occurring in high density scenarios. Extensive numerical simulations show an increased regularity of the resulting trajectories and confirm a general improvement of the model realism.","tags":["journal"],"title":"Walking Ahead: The Headed Social Force Model","type":"publication"},{"authors":["Francesco Farina","Daniele Fontanelli","Andrea Garulli","Antonio Giannitrapani","Domenico Prattichizzo"],"categories":null,"content":"","date":1480546800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1480546800,"objectID":"5c7d4c57cef10cc12f9ca24213b2d09a","permalink":"https://francescofarina.github.io/publication/farina-2016-helbing/","publishdate":"2016-12-01T00:00:00+01:00","relpermalink":"/publication/farina-2016-helbing/","section":"publication","summary":"The increased diffusion of service robots operating in tight collaboration with humans has renewed the interest of the scientific community towards realistic human motion models. In this paper, we present the Headed Social Force Model, a modeling approach enriching Helbing's Social ForceModel with Laumond's human locomotion models. The proposed solution is shown to inherit the best features of either models, being able to reliably reproduce pedestrians' motions both in free space and in highly crowded environments. Extensive numerical simulations are presented in order to evaluate the performance under very different operating conditions.","tags":["conference"],"title":"When Helbing meets Laumond: The Headed Social Force Model","type":"publication"}]