@article{farina2019arXivProximal,
 abstract = {In this paper we introduce a class of novel distributed algorithms for
solving stochastic big-data convex optimization problems over
directed graphs. In the addressed set-up, the dimension of the
decision variable can be extremely high and the objective
function can be nonsmooth. The general algorithm consists of two
main steps: a consensus step and an update on a single block of
the optimization variable, which is then broadcast to neighbors.
Three special instances of the proposed method, involving
particular problem structures, are then presented. In the
general case, the convergence of a dynamic consensus algorithm
over random row stochastic matrices is shown. Then, the
convergence of the proposed algorithm to the optimal cost is
proven in expected value. Exact convergence is achieved when
using diminishing (local) stepsizes, while approximate
convergence is attained when constant stepsizes are employed. In
both cases, the convergence rate is shown to be sublinear.
Finally, a numerical example involving a distributed
classification problem is provided to corroborate the
theoretical results.},
 adsurl = {https://ui.adsabs.harvard.edu/abs/2019arXiv190504214F},
 archiveprefix = {arXiv},
 author = {Farina, Francesco and Notarstefano, Giuseppe},
 eid = {arXiv:1905.04214},
 eprint = {1905.04214},
 journal = {arXiv e-prints},
 keywords = {Mathematics - Optimization and Control},
 month = {May},
 pages = {arXiv:1905.04214},
 primaryclass = {math.OC},
 title = {Randomized Block Proximal Methods for Distributed Stochastic Big-Data Optimization},
 year = {2019}
}

